# =================================================================
# Bedrock Chat FastAPI Configuration Example
# =================================================================
# Copy this file to .env and modify the values as needed.
# All values shown here are the defaults used by the system.

# =================================================================
# REQUIRED: AWS Configuration
# =================================================================
# Set your AWS region where Bedrock is available
AWS_REGION=us-east-1

# AWS Credentials (optional if using IAM roles or AWS CLI profile)
# AWS_ACCESS_KEY_ID=your_access_key_here
# AWS_SECRET_ACCESS_KEY=your_secret_key_here

# =================================================================
# Model Configuration
# =================================================================
# Available Bedrock models:

# Claude Models (Anthropic)
# - us.anthropic.claude-sonnet-4-5-20250929-v1:0 (Claude 4.5 Sonnet - LATEST, most advanced)
# - us.anthropic.claude-3-5-sonnet-20241022-v2:0 (Claude 3.5 Sonnet - with inference profile)
# - anthropic.claude-3-5-sonnet-20241022-v2:0 (Claude 3.5 Sonnet - recommended, most capable)
# - anthropic.claude-3-5-haiku-20241022-v1:0 (Claude 3.5 Haiku - faster, cheaper)
# - anthropic.claude-3-haiku-20240307-v1:0 (Claude 3 Haiku - legacy, cheapest)

# OpenAI GPT OSS Models
# - openai.gpt-oss-120b-1:0 (120B parameter open-source model - excellent performance)

# Meta Llama Models (Excellent for Tool Calling)
# - us.meta.llama3-2-90b-instruct-v1:0 (Llama 3.2 90B - NEWEST, excellent tool calling, cross-region profile)
# - meta.llama3-1-70b-instruct-v1:0 (Llama 3.1 70B - great for structured outputs)
# - meta.llama3-1-8b-instruct-v1:0 (Llama 3.1 8B - fast, good for simple tools)

# Cohere Command Models (Strong Multi-Step Reasoning)
# - cohere.command-r-plus-v1:0 (Command R+ - excellent for complex tool workflows)
# - cohere.command-r-v1:0 (Command R - good balance of speed and capability)

# Other Models
# - amazon.titan-text-express-v1 (Amazon Titan - basic text generation)
# - ai21.jamba-instruct-v1:0 (AI21 Jamba - hybrid architecture, long context)

# Default: Claude 4.5 Sonnet (latest and most advanced)
BEDROCK_MODEL_ID=us.anthropic.claude-sonnet-4-5-20250929-v1:0

# Temperature controls randomness (0.0 = deterministic, 1.0 = very random)
BEDROCK_TEMPERATURE=0.7

# Maximum tokens in model response (affects cost and response length)
BEDROCK_MAX_TOKENS=4096

# Top-p sampling parameter (0.0-1.0)
# PARAMETER COMPATIBILITY:
# - Claude models: Use EITHER temperature OR top_p, not both (validation error if both)
# - OpenAI GPT models: Support BOTH temperature AND top_p simultaneously
# - Other models: Check model-specific documentation
BEDROCK_TOP_P=0.9

# =================================================================
# System Prompt
# =================================================================
# Define the AI assistant's behavior and capabilities
BEDROCK_SYSTEM_PROMPT="You are a helpful AI assistant for this API. Use the available tools to provide accurate information and help users accomplish their tasks."

# =================================================================
# API Endpoints
# =================================================================
# Customize the chat endpoints
BEDROCK_CHAT_ENDPOINT=/bedrock-chat
BEDROCK_WEBSOCKET_ENDPOINT=/bedrock-chat/ws
BEDROCK_UI_ENDPOINT=/bedrock-chat/ui

# Enable/disable the built-in chat UI
BEDROCK_ENABLE_UI=true

# =================================================================
# Tool Configuration
# =================================================================
# Which API paths the AI can access (comma-separated)
# Leave empty to allow all paths except excluded ones
BEDROCK_ALLOWED_PATHS=

# Which API paths to exclude from AI access (comma-separated)
BEDROCK_EXCLUDED_PATHS=/docs,/redoc,/openapi.json,/bedrock-chat

# Maximum tool calls per conversation turn (prevents infinite loops)
BEDROCK_MAX_TOOL_CALLS=10

# Timeout for API calls in seconds
BEDROCK_TIMEOUT=30

# =================================================================
# Session Management
# =================================================================
# Maximum concurrent WebSocket sessions
BEDROCK_MAX_SESSIONS=1000

# Session timeout in seconds (1 hour = 3600)
BEDROCK_SESSION_TIMEOUT=3600

# =================================================================
# Security
# =================================================================
# Rate limiting (examples: "10/minute", "100/hour", "1000/day")
# BEDROCK_RATE_LIMIT=60/minute

# CORS origins (comma-separated, * for all)
BEDROCK_CORS_ORIGINS=*

# =================================================================
# Logging
# =================================================================
# Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
BEDROCK_LOG_LEVEL=INFO

# Log all API calls for debugging (can be verbose)
BEDROCK_LOG_API_CALLS=false

# Log errors
BEDROCK_LOG_ERRORS=true

# =================================================================
# Error Handling & Reliability
# =================================================================
# Maximum retries for failed requests
BEDROCK_MAX_RETRIES=3

# Delay between retries in seconds
BEDROCK_RETRY_DELAY=1.0

# Use exponential backoff for retries
BEDROCK_EXPONENTIAL_BACKOFF=true

# Continue working even if some features fail
BEDROCK_GRACEFUL_DEGRADATION=true

# Fallback model if primary model fails
# BEDROCK_FALLBACK_MODEL=anthropic.claude-3-5-haiku-20241022-v1:0

# =================================================================
# Conversation Management
# =================================================================
# Maximum number of conversation messages to keep in history
# Reduce this if you encounter "Input is too long" errors
# IMPORTANT: For repeated large inputs, use very low values (5-10) to prevent 
# "Failed to buffer the request body: length limit exceeded" errors
# NOTE: GPT OSS models may encounter tokenization errors in conversations >10-12 exchanges
BEDROCK_MAX_CONVERSATION_MESSAGES=20

# Conversation trimming strategy: sliding_window, truncate, smart_prune
# smart_prune removes tool messages first, recommended for large conversations
BEDROCK_CONVERSATION_STRATEGY=smart_prune

# Whether to preserve system message during conversation trimming
BEDROCK_PRESERVE_SYSTEM_MESSAGE=true

# =================================================================
# Message Chunking
# =================================================================
# Enable automatic message chunking for large content
BEDROCK_ENABLE_MESSAGE_CHUNKING=true

# Maximum message size before chunking (in characters)
# Reduce this if you encounter "Input is too long" errors
# For GPT OSS models, consider even smaller values (30000-50000)
BEDROCK_MAX_MESSAGE_SIZE=80000

# Size of each chunk (in characters)
# Smaller chunks help prevent context window issues
# For GPT OSS models, consider smaller chunks (20000-30000)
# IMPORTANT: Very large inputs can create 100+ chunks. For repeated large inputs
# in the same conversation, use much smaller chunks (10000-20000) to prevent 
# HTTP request body size limits
BEDROCK_CHUNK_SIZE=40000

# Message chunking strategy: simple, preserve_context, semantic
BEDROCK_CHUNKING_STRATEGY=preserve_context

# Overlap between chunks for context preservation (in characters)
BEDROCK_CHUNK_OVERLAP=3000

# =================================================================
# Advanced Tool Handling
# =================================================================
# Maximum number of recursive tool call rounds
BEDROCK_MAX_TOOL_CALL_ROUNDS=5

# =================================================================
# Model-Specific Notes
# =================================================================
# GPT OSS Models: Have stricter limits and special considerations:
# - Context window: max_tokens automatically reduced to 1-10 tokens for large inputs
# - Request body size: Large chunked conversations can exceed HTTP body limits  
# - Tokenization issues: May encounter "Unexpected token" errors in longer conversations
# - Repeated large inputs in same conversation create exponentially larger requests
# - Recommendations: smaller chunks (10000-20000), fewer messages (5-10), or start new conversations
# - System automatically applies ultra-aggressive fallback (truncates to ~2-3 messages)
# - For frequent large inputs or long conversations, Claude models handle interactions better
#
# Claude Models: Handle large conversations more gracefully and typically
# don't require max_tokens adjustments or aggressive conversation management.
#
# Meta Llama Models: Excellent for tool calling with strong structured output capabilities.
# - Llama 3.2 90B: Best overall performance for complex tool workflows
# - Llama 3.1 70B: Great balance of capability and speed for most tool calling tasks
# - Handle tool calling reliably with good parameter extraction
#
# Cohere Command Models: Specialized in multi-step reasoning and RAG workflows.
# - Command R+: Excellent for complex tool sequences and retrieval tasks
# - Command R: Good for standard tool calling with efficient performance

# =================================================================
# Environment-Specific Overrides
# =================================================================

# Development settings (faster/cheaper models for testing)
# BEDROCK_MODEL_ID=anthropic.claude-3-5-haiku-20241022-v1:0
# BEDROCK_TEMPERATURE=0.3
# BEDROCK_LOG_API_CALLS=true
# BEDROCK_LOG_LEVEL=DEBUG

# Production settings (latest models with security)
# BEDROCK_MODEL_ID=us.anthropic.claude-sonnet-4-5-20250929-v1:0
# BEDROCK_ENABLE_UI=false
# BEDROCK_RATE_LIMIT=30/minute
# BEDROCK_LOG_API_CALLS=false
# BEDROCK_MAX_SESSIONS=500

# Testing settings (deterministic responses)
# BEDROCK_MODEL_ID=openai.gpt-oss-120b-1:0
# BEDROCK_TEMPERATURE=0.0
# BEDROCK_MAX_TOOL_CALLS=3
# BEDROCK_TIMEOUT=10